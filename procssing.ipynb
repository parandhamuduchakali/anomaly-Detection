{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72eb6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INDUSTRIAL ANOMALY DETECTION - REFINED IMPLEMENTATION\n",
      "================================================================================\n",
      "Starting Industrial Anomaly Detection Pipeline...\n",
      "\n",
      "1. LOADING AND CLEANING DATA\n",
      "--------------------------------------------------\n",
      "✓ Dataset loaded successfully: (20000, 96)\n",
      "✓ Timestamp created and data sorted chronologically\n",
      "✓ No missing values detected\n",
      "\n",
      "2. FEATURE ENGINEERING\n",
      "--------------------------------------------------\n",
      "✓ Time-based features created\n",
      "✓ Created 19 new features:\n",
      "  - Efficiency: PickEfficiency, DepositEfficiency\n",
      "  - Deviations: ActuatorPosDeviation, ActuatorPosDeviation_Abs, BellowPressureDeviation...\n",
      "  - Health: VacuumEfficiency, TempDeviation, TotalAlarms...\n",
      "  - Trends: CycleTime_MA5, CycleTime_Std5\n",
      "\n",
      "3. PREPARING MODELING DATA\n",
      "--------------------------------------------------\n",
      "Target column 'Alarm.ItemDroppedError' distribution:\n",
      "Alarm.ItemDroppedError\n",
      "0.0    20000\n",
      "Name: count, dtype: int64\n",
      "✓ Removed 102 constant/near-constant features\n",
      "✓ Final feature matrix: (20000, 3)\n",
      "✓ Modeling approach: Unsupervised only\n",
      "\n",
      "4. TRAINING ANOMALY DETECTION MODELS\n",
      "--------------------------------------------------\n",
      "Training unsupervised models...\n",
      "  ✓ Isolation Forest trained\n",
      "  ✓ DBSCAN clustering completed\n",
      "  ✓ Statistical analysis completed\n",
      "\n",
      "5. GENERATING ANOMALY PREDICTIONS\n",
      "--------------------------------------------------\n",
      "  ✓ Isolation Forest: 204 anomalies (1.02%)\n",
      "  ✓ DBSCAN: 0 anomalies (0.00%)\n",
      "  ✓ Statistical (3σ): 35 anomalies (0.18%)\n",
      "  ✓ Ensemble: 35 anomalies (0.18%)\n",
      "\n",
      "6. RESULTS ANALYSIS\n",
      "--------------------------------------------------\n",
      "Anomaly Detection Summary:\n",
      "  isolation_forest: 1.02%\n",
      "  dbscan: 0.00%\n",
      "  statistical: 0.18%\n",
      "  ensemble: 0.18%\n",
      "\n",
      "Temporal Analysis:\n",
      "  First anomaly: 2019-09-25 06:55:41.718000\n",
      "  Last anomaly: 2019-09-25 06:55:42.058000\n",
      "  Peak anomaly hour: 6:00\n",
      "\n",
      "Top 5 features correlated with anomalies:\n",
      "  IO.FilterPressure: 1.000\n",
      "  Minute: 0.078\n",
      "  Relative time: 0.072\n",
      "  IO.ToolTemperature: 0.062\n",
      "  TempDeviation: 0.059\n",
      "\n",
      "7. BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "Operational Summary:\n",
      "  • Total operations analyzed: 20,000\n",
      "  • Potential anomalies detected: 35\n",
      "  • Anomaly rate: 0.18%\n",
      "\n",
      "Risk Assessment:\n",
      "  • Risk level: LOW\n",
      "  • Recommended action: Continue monitoring\n",
      "\n",
      "Operational Metrics:\n",
      "  • Analysis duration: 0.1 hours\n",
      "  • Operations per hour: 360018.0\n",
      "  • Anomalies per hour: 630.0\n",
      "\n",
      "Recommendations:\n",
      "  1. Implement real-time monitoring using ensemble model\n",
      "  2. Set alert threshold at ensemble score > 0.7\n",
      "  3. Investigate top correlated features for root causes\n",
      "  4. Schedule preventive maintenance based on patterns\n",
      "  5. Review 35 flagged operations for validation\n",
      "✓ Model saved to models/refined_anomaly_model.pkl\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Industrial Anomaly Detection - Refined and Error-Free Implementation\n",
    "Addresses all identified issues in the original notebook\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INDUSTRIAL ANOMALY DETECTION - REFINED IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and clean the industrial dataset with robust error handling\n",
    "    \"\"\"\n",
    "    print(\"\\n1. LOADING AND CLEANING DATA\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✓ Dataset loaded successfully: {df.shape}\")\n",
    "        \n",
    "        # Handle timestamp creation\n",
    "        if 'Date' in df.columns and 'Time' in df.columns:\n",
    "            # Clean time format (remove trailing ',0')\n",
    "            df['Time'] = df['Time'].astype(str).str.replace(',0', '', regex=False)\n",
    "            \n",
    "            # Create timestamp with error handling\n",
    "            try:\n",
    "                df['Timestamp'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], \n",
    "                                                errors='coerce')\n",
    "                df = df.sort_values('Timestamp').reset_index(drop=True)\n",
    "                print(\"✓ Timestamp created and data sorted chronologically\")\n",
    "                \n",
    "                # Drop original Date and Time columns\n",
    "                df = df.drop(columns=['Date', 'Time'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Warning: Could not create timestamp: {e}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_counts = df.isnull().sum()\n",
    "        if missing_counts.sum() > 0:\n",
    "            print(f\"⚠ Missing values found in {(missing_counts > 0).sum()} columns\")\n",
    "            # Forward fill then backward fill\n",
    "            df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            print(\"✓ Missing values handled\")\n",
    "        else:\n",
    "            print(\"✓ No missing values detected\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create domain-specific features for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"\\n2. FEATURE ENGINEERING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    original_features = df.shape[1]\n",
    "    \n",
    "    # Time-based features (if timestamp exists)\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Hour'] = df['Timestamp'].dt.hour\n",
    "        df['Minute'] = df['Timestamp'].dt.minute\n",
    "        df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "        print(\"✓ Time-based features created\")\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    efficiency_features = []\n",
    "    if all(col in df.columns for col in ['CompletedLog.ToolPickTime', 'CompletedLog.TotalCycleTime']):\n",
    "        df['PickEfficiency'] = df['CompletedLog.ToolPickTime'] / (df['CompletedLog.TotalCycleTime'] + 1e-6)\n",
    "        efficiency_features.append('PickEfficiency')\n",
    "        \n",
    "    if all(col in df.columns for col in ['CompletedLog.ToolDepositTime', 'CompletedLog.TotalCycleTime']):\n",
    "        df['DepositEfficiency'] = df['CompletedLog.ToolDepositTime'] / (df['CompletedLog.TotalCycleTime'] + 1e-6)\n",
    "        efficiency_features.append('DepositEfficiency')\n",
    "    \n",
    "    # Deviation features (actual vs target)\n",
    "    deviation_features = []\n",
    "    deviation_pairs = [\n",
    "        ('Actuator.TargetPos', 'Actuator.CurrentPos', 'ActuatorPosDeviation'),\n",
    "        ('Actuator.WantedBellowPressure', 'IO.BellowPressure', 'BellowPressureDeviation'),\n",
    "        ('VacuumBlower.CMD.SpeedHz', 'Vacuum.Stat.ActSpeedHz', 'VacuumSpeedDeviation')\n",
    "    ]\n",
    "    \n",
    "    for target_col, actual_col, dev_name in deviation_pairs:\n",
    "        if target_col in df.columns and actual_col in df.columns:\n",
    "            df[dev_name] = df[target_col] - df[actual_col]\n",
    "            df[f'{dev_name}_Abs'] = np.abs(df[dev_name])\n",
    "            deviation_features.extend([dev_name, f'{dev_name}_Abs'])\n",
    "    \n",
    "    # System health indicators\n",
    "    health_features = []\n",
    "    \n",
    "    # Vacuum system health\n",
    "    if 'IO.SoleVacuum' in df.columns and 'IO.CurrentVacuumMotor' in df.columns:\n",
    "        df['VacuumEfficiency'] = df['IO.SoleVacuum'] / (df['IO.CurrentVacuumMotor'] + 1e-6)\n",
    "        health_features.append('VacuumEfficiency')\n",
    "    \n",
    "    # Temperature stability\n",
    "    if 'IO.ToolTemperature' in df.columns:\n",
    "        temp_mean = df['IO.ToolTemperature'].mean()\n",
    "        df['TempDeviation'] = np.abs(df['IO.ToolTemperature'] - temp_mean)\n",
    "        health_features.append('TempDeviation')\n",
    "    \n",
    "    # Alarm aggregation\n",
    "    alarm_cols = [col for col in df.columns if 'Alarm.' in col]\n",
    "    if alarm_cols:\n",
    "        df['TotalAlarms'] = df[alarm_cols].sum(axis=1)\n",
    "        df['SystemHealthScore'] = 1 - (df['TotalAlarms'] / len(alarm_cols))\n",
    "        health_features.extend(['TotalAlarms', 'SystemHealthScore'])\n",
    "    \n",
    "    # Machine status\n",
    "    if 'Status.MachineStatus' in df.columns:\n",
    "        df['MachineRunning'] = (df['Status.MachineStatus'] > 0).astype(int)\n",
    "        health_features.append('MachineRunning')\n",
    "    \n",
    "    # Rolling statistics for trend detection\n",
    "    trend_features = []\n",
    "    if 'CompletedLog.TotalCycleTime' in df.columns:\n",
    "        df['CycleTime_MA5'] = df['CompletedLog.TotalCycleTime'].rolling(5, min_periods=1).mean()\n",
    "        df['CycleTime_Std5'] = df['CompletedLog.TotalCycleTime'].rolling(5, min_periods=1).std().fillna(0)\n",
    "        trend_features.extend(['CycleTime_MA5', 'CycleTime_Std5'])\n",
    "    \n",
    "    new_features = df.shape[1] - original_features\n",
    "    print(f\"✓ Created {new_features} new features:\")\n",
    "    if efficiency_features:\n",
    "        print(f\"  - Efficiency: {', '.join(efficiency_features)}\")\n",
    "    if deviation_features:\n",
    "        print(f\"  - Deviations: {', '.join(deviation_features[:3])}...\")\n",
    "    if health_features:\n",
    "        print(f\"  - Health: {', '.join(health_features[:3])}...\")\n",
    "    if trend_features:\n",
    "        print(f\"  - Trends: {', '.join(trend_features)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATA PREPARATION FOR MODELING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_modeling_data(df, target_col='Alarm.ItemDroppedError'):\n",
    "    \"\"\"\n",
    "    Prepare clean feature matrix for machine learning\n",
    "    \"\"\"\n",
    "    print(\"\\n3. PREPARING MODELING DATA\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check target column\n",
    "    target_exists = target_col in df.columns\n",
    "    if target_exists:\n",
    "        target_stats = df[target_col].value_counts()\n",
    "        print(f\"Target column '{target_col}' distribution:\")\n",
    "        print(target_stats)\n",
    "        has_positive_cases = (df[target_col] > 0).any()\n",
    "    else:\n",
    "        print(f\"⚠ Target column '{target_col}' not found - using unsupervised approach\")\n",
    "        has_positive_cases = False\n",
    "    \n",
    "    # Select features for modeling\n",
    "    exclude_cols = [\n",
    "        'Relative time', 'Timestamp', 'Hour', 'Minute', 'DayOfWeek', 'IsWeekend',\n",
    "        'Statistics.SequenceNr', 'BellowSoftTouch.SequenceNo'  # ID columns\n",
    "    ]\n",
    "    \n",
    "    # Add target column to exclusions if it exists\n",
    "    if target_exists:\n",
    "        exclude_cols.append(target_col)\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    # Handle any remaining missing values\n",
    "    X = X.fillna(X.median()).fillna(0)\n",
    "    \n",
    "    # Remove constant or near-constant features\n",
    "    constant_mask = []\n",
    "    for col in X.columns:\n",
    "        if X[col].nunique() <= 1 or X[col].std() < 1e-8:\n",
    "            constant_mask.append(col)\n",
    "    \n",
    "    if constant_mask:\n",
    "        X = X.drop(columns=constant_mask)\n",
    "        print(f\"✓ Removed {len(constant_mask)} constant/near-constant features\")\n",
    "    \n",
    "    # Prepare target variable\n",
    "    if target_exists and has_positive_cases:\n",
    "        y = df[target_col].astype(int)\n",
    "        supervised_possible = True\n",
    "    else:\n",
    "        y = None\n",
    "        supervised_possible = False\n",
    "    \n",
    "    print(f\"✓ Final feature matrix: {X.shape}\")\n",
    "    print(f\"✓ Modeling approach: {'Supervised + Unsupervised' if supervised_possible else 'Unsupervised only'}\")\n",
    "    \n",
    "    return X, y, supervised_possible\n",
    "\n",
    "# =============================================================================\n",
    "# 4. ANOMALY DETECTION MODELS\n",
    "# =============================================================================\n",
    "\n",
    "class RefinedAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Comprehensive anomaly detection with multiple algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scaler = None\n",
    "        self.feature_names = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def fit(self, X, y=None, supervised_possible=False):\n",
    "        \"\"\"\n",
    "        Train anomaly detection models\n",
    "        \"\"\"\n",
    "        print(\"\\n4. TRAINING ANOMALY DETECTION MODELS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = RobustScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Train unsupervised models (always applicable)\n",
    "        self._train_unsupervised(X_scaled)\n",
    "        \n",
    "        # Train supervised models if possible\n",
    "        if supervised_possible and y is not None:\n",
    "            self._train_supervised(X_scaled, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_unsupervised(self, X_scaled):\n",
    "        \"\"\"Train unsupervised anomaly detection models\"\"\"\n",
    "        print(\"Training unsupervised models...\")\n",
    "        \n",
    "        # 1. Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=0.05,  # Expect 5% anomalies\n",
    "            random_state=42,\n",
    "            n_estimators=200,\n",
    "            max_samples='auto'\n",
    "        )\n",
    "        iso_forest.fit(X_scaled)\n",
    "        self.models['isolation_forest'] = iso_forest\n",
    "        print(\"  ✓ Isolation Forest trained\")\n",
    "        \n",
    "        # 2. DBSCAN for density-based anomalies\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "        cluster_labels = dbscan.fit_predict(X_scaled)\n",
    "        self.models['dbscan'] = dbscan\n",
    "        self.models['dbscan_labels'] = cluster_labels\n",
    "        print(\"  ✓ DBSCAN clustering completed\")\n",
    "        \n",
    "        # 3. Statistical Z-score method\n",
    "        z_scores = np.abs(zscore(X_scaled, axis=0, nan_policy='omit'))\n",
    "        max_z_scores = np.nanmax(z_scores, axis=1)\n",
    "        self.models['z_scores'] = max_z_scores\n",
    "        print(\"  ✓ Statistical analysis completed\")\n",
    "    \n",
    "    def _train_supervised(self, X_scaled, y):\n",
    "        \"\"\"Train supervised models when positive cases exist\"\"\"\n",
    "        print(\"Training supervised models...\")\n",
    "        \n",
    "        try:\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Random Forest Classifier\n",
    "            rf_clf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            rf_clf.fit(X_train, y_train)\n",
    "            self.models['random_forest'] = rf_clf\n",
    "            self.models['test_data'] = (X_test, y_test)\n",
    "            print(\"  ✓ Random Forest trained\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Supervised training failed: {e}\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Generate anomaly predictions using ensemble of models\n",
    "        \"\"\"\n",
    "        print(\"\\n5. GENERATING ANOMALY PREDICTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if X is None:\n",
    "            print(\"No data provided for prediction\")\n",
    "            return None\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        results = {}\n",
    "        \n",
    "        # Isolation Forest predictions\n",
    "        if 'isolation_forest' in self.models:\n",
    "            iso_pred = self.models['isolation_forest'].predict(X_scaled)\n",
    "            iso_scores = self.models['isolation_forest'].decision_function(X_scaled)\n",
    "            results['isolation_forest'] = {\n",
    "                'predictions': (iso_pred == -1).astype(int),\n",
    "                'scores': iso_scores,\n",
    "                'anomaly_rate': (iso_pred == -1).mean() * 100\n",
    "            }\n",
    "            print(f\"  ✓ Isolation Forest: {(iso_pred == -1).sum()} anomalies ({(iso_pred == -1).mean()*100:.2f}%)\")\n",
    "        \n",
    "        # DBSCAN predictions\n",
    "        if 'dbscan_labels' in self.models:\n",
    "            dbscan_labels = self.models['dbscan'].fit_predict(X_scaled)\n",
    "            noise_mask = dbscan_labels == -1\n",
    "            results['dbscan'] = {\n",
    "                'predictions': noise_mask.astype(int),\n",
    "                'anomaly_rate': noise_mask.mean() * 100,\n",
    "                'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "            }\n",
    "            print(f\"  ✓ DBSCAN: {noise_mask.sum()} anomalies ({noise_mask.mean()*100:.2f}%)\")\n",
    "        \n",
    "        # Statistical anomalies\n",
    "        if 'z_scores' in self.models:\n",
    "            z_scores = np.abs(zscore(X_scaled, axis=0, nan_policy='omit'))\n",
    "            max_z_scores = np.nanmax(z_scores, axis=1)\n",
    "            z_threshold = 3.0  # 3-sigma rule\n",
    "            z_anomalies = max_z_scores > z_threshold\n",
    "            results['statistical'] = {\n",
    "                'predictions': z_anomalies.astype(int),\n",
    "                'scores': max_z_scores,\n",
    "                'anomaly_rate': z_anomalies.mean() * 100\n",
    "            }\n",
    "            print(f\"  ✓ Statistical (3σ): {z_anomalies.sum()} anomalies ({z_anomalies.mean()*100:.2f}%)\")\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_scores = np.zeros(len(X_scaled))\n",
    "        weight_sum = 0\n",
    "        \n",
    "        if 'isolation_forest' in results:\n",
    "            ensemble_scores += results['isolation_forest']['predictions'] * 0.4\n",
    "            weight_sum += 0.4\n",
    "            \n",
    "        if 'dbscan' in results:\n",
    "            ensemble_scores += results['dbscan']['predictions'] * 0.3\n",
    "            weight_sum += 0.3\n",
    "            \n",
    "        if 'statistical' in results:\n",
    "            ensemble_scores += results['statistical']['predictions'] * 0.3\n",
    "            weight_sum += 0.3\n",
    "        \n",
    "        if weight_sum > 0:\n",
    "            ensemble_scores /= weight_sum\n",
    "            ensemble_predictions = (ensemble_scores >= 0.5).astype(int)\n",
    "            results['ensemble'] = {\n",
    "                'predictions': ensemble_predictions,\n",
    "                'scores': ensemble_scores,\n",
    "                'anomaly_rate': ensemble_predictions.mean() * 100\n",
    "            }\n",
    "            print(f\"  ✓ Ensemble: {ensemble_predictions.sum()} anomalies ({ensemble_predictions.mean()*100:.2f}%)\")\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        model_data = {\n",
    "            'models': self.models,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'metadata': {\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'n_features': len(self.feature_names) if self.feature_names else 0,\n",
    "                'model_types': list(self.models.keys())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"✓ Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.models = model_data['models']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "        \n",
    "        print(f\"✓ Model loaded from {filepath}\")\n",
    "        print(f\"  Features: {len(self.feature_names)}\")\n",
    "        print(f\"  Model types: {', '.join(model_data['metadata']['model_types'])}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. EVALUATION AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_results(df, results, target_col='Alarm.ItemDroppedError'):\n",
    "    \"\"\"\n",
    "    Analyze and interpret anomaly detection results\n",
    "    \"\"\"\n",
    "    print(\"\\n6. RESULTS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    for method_name, method_results in results.items():\n",
    "        if 'predictions' in method_results:\n",
    "            df[f'Anomaly_{method_name}'] = method_results['predictions']\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"Anomaly Detection Summary:\")\n",
    "    for method_name, method_results in results.items():\n",
    "        if 'anomaly_rate' in method_results:\n",
    "            print(f\"  {method_name}: {method_results['anomaly_rate']:.2f}%\")\n",
    "    \n",
    "    # Temporal analysis (if timestamp available)\n",
    "    if 'Timestamp' in df.columns and 'ensemble' in results:\n",
    "        anomaly_mask = results['ensemble']['predictions'] == 1\n",
    "        if anomaly_mask.sum() > 0:\n",
    "            anomaly_data = df[anomaly_mask]\n",
    "            print(f\"\\nTemporal Analysis:\")\n",
    "            print(f\"  First anomaly: {anomaly_data['Timestamp'].min()}\")\n",
    "            print(f\"  Last anomaly: {anomaly_data['Timestamp'].max()}\")\n",
    "            \n",
    "            if 'Hour' in df.columns:\n",
    "                hourly_dist = anomaly_data['Hour'].value_counts().sort_index()\n",
    "                if len(hourly_dist) > 0:\n",
    "                    peak_hour = hourly_dist.idxmax()\n",
    "                    print(f\"  Peak anomaly hour: {peak_hour}:00\")\n",
    "    \n",
    "    # Feature correlation analysis\n",
    "    if 'ensemble' in results:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            correlations = []\n",
    "            ensemble_predictions = results['ensemble']['predictions']\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col not in [f'Anomaly_{name}' for name in results.keys()]:\n",
    "                    try:\n",
    "                        corr = np.corrcoef(df[col].fillna(0), ensemble_predictions)[0, 1]\n",
    "                        if not np.isnan(corr):\n",
    "                            correlations.append((col, abs(corr)))\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(f\"\\nTop 5 features correlated with anomalies:\")\n",
    "            for feature, corr in correlations[:5]:\n",
    "                print(f\"  {feature}: {corr:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. BUSINESS INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_business_insights(df, results):\n",
    "    \"\"\"\n",
    "    Generate actionable business insights\n",
    "    \"\"\"\n",
    "    print(\"\\n7. BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_operations = len(df)\n",
    "    \n",
    "    if 'ensemble' in results:\n",
    "        anomaly_count = results['ensemble']['predictions'].sum()\n",
    "        anomaly_rate = results['ensemble']['anomaly_rate']\n",
    "    else:\n",
    "        anomaly_count = 0\n",
    "        anomaly_rate = 0\n",
    "    \n",
    "    print(f\"Operational Summary:\")\n",
    "    print(f\"  • Total operations analyzed: {total_operations:,}\")\n",
    "    print(f\"  • Potential anomalies detected: {anomaly_count:,}\")\n",
    "    print(f\"  • Anomaly rate: {anomaly_rate:.2f}%\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    if anomaly_rate < 2:\n",
    "        risk_level = \"LOW\"\n",
    "        action = \"Continue monitoring\"\n",
    "    elif anomaly_rate < 5:\n",
    "        risk_level = \"MEDIUM\"\n",
    "        action = \"Investigate patterns\"\n",
    "    else:\n",
    "        risk_level = \"HIGH\"\n",
    "        action = \"Immediate investigation required\"\n",
    "    \n",
    "    print(f\"\\nRisk Assessment:\")\n",
    "    print(f\"  • Risk level: {risk_level}\")\n",
    "    print(f\"  • Recommended action: {action}\")\n",
    "    \n",
    "    # Time-based insights\n",
    "    if 'Timestamp' in df.columns:\n",
    "        duration = (df['Timestamp'].max() - df['Timestamp'].min()).total_seconds() / 3600\n",
    "        operations_per_hour = total_operations / duration if duration > 0 else 0\n",
    "        anomalies_per_hour = anomaly_count / duration if duration > 0 else 0\n",
    "        \n",
    "        print(f\"\\nOperational Metrics:\")\n",
    "        print(f\"  • Analysis duration: {duration:.1f} hours\")\n",
    "        print(f\"  • Operations per hour: {operations_per_hour:.1f}\")\n",
    "        print(f\"  • Anomalies per hour: {anomalies_per_hour:.1f}\")\n",
    "    \n",
    "    print(f\"\\nRecommendations:\")\n",
    "    print(f\"  1. Implement real-time monitoring using ensemble model\")\n",
    "    print(f\"  2. Set alert threshold at ensemble score > 0.7\")\n",
    "    print(f\"  3. Investigate top correlated features for root causes\")\n",
    "    print(f\"  4. Schedule preventive maintenance based on patterns\")\n",
    "    \n",
    "    if anomaly_count > 0:\n",
    "        print(f\"  5. Review {anomaly_count} flagged operations for validation\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main_pipeline(file_path, target_col='Alarm.ItemDroppedError', save_model=True):\n",
    "    \"\"\"\n",
    "    Execute the complete anomaly detection pipeline\n",
    "    \"\"\"\n",
    "    print(\"Starting Industrial Anomaly Detection Pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and clean data\n",
    "        df = load_and_clean_data(file_path)\n",
    "        \n",
    "        # Step 2: Feature engineering\n",
    "        df = engineer_features(df)\n",
    "        \n",
    "        # Step 3: Prepare modeling data\n",
    "        X, y, supervised_possible = prepare_modeling_data(df, target_col)\n",
    "        \n",
    "        # Step 4: Train models\n",
    "        detector = RefinedAnomalyDetector()\n",
    "        detector.fit(X, y, supervised_possible)\n",
    "        \n",
    "        # Step 5: Generate predictions\n",
    "        results = detector.predict(X)\n",
    "        \n",
    "        # Step 6: Analyze results\n",
    "        analyze_results(df, results, target_col)\n",
    "        \n",
    "        # Step 7: Business insights\n",
    "        generate_business_insights(df, results)\n",
    "        \n",
    "        # Step 8: Save model\n",
    "        if save_model:\n",
    "            model_path = \"models/refined_anomaly_model.pkl\"\n",
    "            detector.save_model(model_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return detector, df, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 9. MODEL TESTING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def test_saved_model(model_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Test a saved model on new data\n",
    "    \"\"\"\n",
    "    print(\"\\nTESTING SAVED MODEL\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model\n",
    "        detector = RefinedAnomalyDetector()\n",
    "        detector.load_model(model_path)\n",
    "        \n",
    "        # Load new test data\n",
    "        df_test = load_and_clean_data(test_data_path)\n",
    "        df_test = engineer_features(df_test)\n",
    "        \n",
    "        # Prepare features (ensure same features as training)\n",
    "        test_features = [col for col in detector.feature_names if col in df_test.columns]\n",
    "        missing_features = [col for col in detector.feature_names if col not in df_test.columns]\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"⚠ Missing features in test data: {len(missing_features)}\")\n",
    "            # Create missing features with zeros\n",
    "            for feature in missing_features:\n",
    "                df_test[feature] = 0\n",
    "        \n",
    "        X_test = df_test[detector.feature_names]\n",
    "        \n",
    "        # Generate predictions\n",
    "        results = detector.predict(X_test)\n",
    "        \n",
    "        # Add predictions to test dataframe\n",
    "        if results and 'ensemble' in results:\n",
    "            df_test['Anomaly_Prediction'] = results['ensemble']['predictions']\n",
    "            df_test['Anomaly_Score'] = results['ensemble']['scores']\n",
    "        \n",
    "        print(\"✓ Testing completed successfully\")\n",
    "        return df_test, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Testing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# =============================================================================\n",
    "# 10. EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example file path - update with your actual path\n",
    "    sample_path = r\"C:\\Users\\paran_chakali\\projects\\New folder\\AICD_sample.csv\"\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    detector, df_processed, prediction_results = main_pipeline(\n",
    "        file_path=sample_path,\n",
    "        target_col='Alarm.ItemDroppedError',\n",
    "        save_model=True\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd1947",
   "metadata": {},
   "source": [
    "### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe31a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING SAVED MODEL\n",
      "--------------------------------------------------\n",
      "✓ Model loaded from models/refined_anomaly_model.pkl\n",
      "  Features: 3\n",
      "  Model types: isolation_forest, dbscan, dbscan_labels, z_scores\n",
      "\n",
      "1. LOADING AND CLEANING DATA\n",
      "--------------------------------------------------\n",
      "✓ Dataset loaded successfully: (20000, 96)\n",
      "✓ Timestamp created and data sorted chronologically\n",
      "✓ No missing values detected\n",
      "\n",
      "2. FEATURE ENGINEERING\n",
      "--------------------------------------------------\n",
      "✓ Time-based features created\n",
      "✓ Created 19 new features:\n",
      "  - Efficiency: PickEfficiency, DepositEfficiency\n",
      "  - Deviations: ActuatorPosDeviation, ActuatorPosDeviation_Abs, BellowPressureDeviation...\n",
      "  - Health: VacuumEfficiency, TempDeviation, TotalAlarms...\n",
      "  - Trends: CycleTime_MA5, CycleTime_Std5\n",
      "\n",
      "5. GENERATING ANOMALY PREDICTIONS\n",
      "--------------------------------------------------\n",
      "  ✓ Isolation Forest: 204 anomalies (1.02%)\n",
      "  ✓ DBSCAN: 0 anomalies (0.00%)\n",
      "  ✓ Statistical (3σ): 35 anomalies (0.18%)\n",
      "  ✓ Ensemble: 35 anomalies (0.18%)\n",
      "✓ Testing completed successfully\n"
     ]
    }
   ],
   "source": [
    "sample_path = r\"C:\\Users\\paran_chakali\\projects\\New folder\\AICD_sample.csv\"\n",
    "\n",
    "test_df, test_results = test_saved_model(\n",
    "        \"models/refined_anomaly_model.pkl\",\n",
    "        sample_path  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d6b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Prediction\n",
      "0            1\n",
      "1            1\n",
      "2            1\n",
      "3            1\n",
      "4            1\n",
      "5            1\n",
      "6            1\n",
      "7            1\n",
      "8            1\n",
      "9            1\n",
      "10           1\n",
      "11           1\n",
      "12           1\n",
      "13           1\n",
      "14           1\n",
      "15           1\n",
      "16           1\n",
      "17           1\n",
      "18           1\n",
      "19           1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "with open(\"models/anomaly_model_v1.pkl\", \"rb\") as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "model = saved[\"model\"]\n",
    "scaler = saved[\"scaler\"]\n",
    "feature_names = saved[\"features\"]\n",
    "\n",
    "# Load new dataset\n",
    "file_path = r\"C:\\Users\\paran_chakali\\projects\\New folder\\AICD_sample.csv\"\n",
    "df_new = pd.read_csv(file_path)\n",
    "\n",
    "# Drop same unused cols\n",
    "drop_cols = ['Relative time', 'Date', 'Time', 'Timestamp']\n",
    "for col in drop_cols:\n",
    "    if col in df_new.columns:\n",
    "        df_new = df_new.drop(columns=col)\n",
    "\n",
    "# Ensure compatibility with training features\n",
    "X_new = df_new[feature_names].fillna(0)\n",
    "\n",
    "# Scale\n",
    "X_scaled_new = scaler.transform(X_new)\n",
    "\n",
    "# Predict (-1 = anomaly, 1 = normal)\n",
    "df_new[\"Prediction\"] = model.predict(X_scaled_new)\n",
    "\n",
    "# Show first results\n",
    "print(df_new[[\"Prediction\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b96c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
